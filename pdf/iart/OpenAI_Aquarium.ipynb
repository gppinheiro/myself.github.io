{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Puzzle Aquarium - Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authors\n",
    " * Afonso Sá  up201604605\n",
    " * Guilherme Pinheiro  up201703867\n",
    " * Miguel Natal  up201809216\n",
    " \n",
    " ¹ FEUP-IART, Group 55\n",
    " \n",
    " ² Faculdade de Engenharia da Universidade do Porto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index\n",
    "\n",
    "***1. [Introduction](#Introduction)***\n",
    "\n",
    "***2. [Environments](#Environments)***\n",
    "\n",
    "***3. [RL Algorithms](#RLAlgorithms)***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aquarium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The puzzle is played on a rectangular grid divided into blocks called \"aquariums\".\n",
    "- The players has to \"fill\" the aquariums with water up to a certain level or leave it empty.\n",
    "- The water level in each aquarium is one and the same across its full width.\n",
    "- The numbers outside the grid show the number of filled cells horizontally and vertically.\n",
    "\n",
    "\n",
    "Example of an unsolved and solved puzzle:\n",
    "<table><tr>\n",
    "    <td>\n",
    "        <img src=\"images/aquarium.png\" width=\"250px\" alt=\"Aquarium To Be Solve\"/>\n",
    "    </td>\n",
    "    <td>\n",
    "        <img src=\"images/aquarium_solved.png\" width=\"250px\" alt=\"Aquarium Solved\"/>\n",
    "    </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is reinforcement learning? Why do we need it?\n",
    "\n",
    "Reinforcement learning (RL) is a machine learning technique that focuses on training an algorithm following the cut-and-try approach. The algorithm (agent) evaluates a current situation (state), takes an action, and receives feedback (reward) from the environment after each act. \n",
    "\n",
    "In our case, we use both, negative and positive rewards. For our case, a positive reward tells the agent the game is over (reached the final solution), and a negative feedback is a punishment for making a mistake, in our case if it fills the board with more water than what the H and V objective matrices defined.\n",
    "\n",
    "The reinforcement learning algorithm learns how to act best through many attempts and failures. Trial-and-error learning is connected with the so-called long-term reward. This reward is the ultimate goal the agent learns while interacting with an environment through numerous trials and errors. The algorithm gets short-term rewards that together lead to the cumulative, long-term one.\n",
    "\n",
    "So, the key goal of reinforcement learning used today is to define the best sequence of decisions that allow the agent to solve a problem while maximizing a long-term reward. And that set of coherent actions is learned through the interaction with environment and observation of rewards in every state.\n",
    "\n",
    "On the whole, reinforcement learning algorithms will learn and play the game easily. So, the main goal is to build an agent that is capable of solving the aquarium puzzle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools\n",
    "\n",
    "* Programing language: Python\n",
    "* Development environment: VS Code\n",
    "* Libarires needed for development: Pygame, OpenAI Gym \n",
    "* Library for PPO: https://stable-baselines3.readthedocs.io/en/master/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install gym\n",
    "!{sys.executable} -m pip install pygame\n",
    "!{sys.executable} -m pip install pygame\n",
    "!{sys.executable} -m pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environments\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have implemented six different environments in order to test how different rewards would affect the learning process of our agent. We used puzzles with a size of 3x3 and 4x4 for these tests. A board whose size is bigger than 4 is computacionally heavy and needed more episodes for the RL agent to find a solution.\n",
    "\n",
    "<b>Action Space: </b>In order to solve our puzzle the program can choose between n actions, where n is the number of aquariums (containers) present in each puzzle. This actions are represented with an integer that goes from 0 to n-1 and they fill the specified container one more level (if possible).\n",
    "\n",
    "<b>Observation Space: </b>Firstly (on the \"Aquarium-v0\"), as a proof of concept we used an over-simplified observation space where each state variable contained the level of water in each aquarium. With no extra info, our algorithms were basically playing a guessing game but testing like this made sure we had everything implemented right, specially when we stopped generating the boards randomly and we could see it \"learn\".\n",
    "Afterwards (on every other environment), we struggled to get a generic representation of each different board and finding the balance between giving too much information or too little. If we gave our algorithm the whole board, objectives and aquarium levels, it would learn each specific game and it would only be helpful if we were asked to solve that same board again. Too little information and the algorithm would be playing a guessing game. In our solution we tried to abstract the observation space from the specific board but tried to give enough information for the agent to base their actions, although not perfect, provided satisfactory results. Our approach calculates the number of water squares missing to reach each objective (collumns and rows) and gives this information en block with the depth of water in each container."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aquarium-v0\n",
    " \n",
    "<table align=\"left\" style=\"width: 15em; margin: 1em 2em 2em 2em; border: 3px solid #f8f8f8;\">\n",
    "    <tr>\n",
    "        <th style=\"text-align: center;\">Action</th>\n",
    "        <th style=\"text-align: center;\">Reward</th>\n",
    "    <tr>\n",
    "        <td style=\"text-align: center\">Finish</td>\n",
    "        <td style=\"text-align: center\">+50</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: center\">OverFlow</td>\n",
    "        <td style=\"text-align: center\">-50</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: center\">Diff between stateSOL and stateUser</td>\n",
    "        <td style=\"text-align: center\">-10</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Board: 4 x 4\n",
    "\n",
    "Number of Aquariums: 4\n",
    "\n",
    "This envirnoment was used only to test our algorithms, nothing more. The logic for this has the name \"game_logic_old.py\" ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aquarium-v1\n",
    " \n",
    "<table align=\"left\" style=\"width: 15em; margin: 1em 2em 2em 2em; border: 3px solid #f8f8f8;\">\n",
    "    <tr>\n",
    "        <th style=\"text-align: center\">Action</th>\n",
    "        <th style=\"text-align: center\">Reward</th>\n",
    "    <tr>\n",
    "        <td style=\"text-align: center\">Finish</td>\n",
    "        <td style=\"text-align: center\">+50</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: center\">OverFlow</td>\n",
    "        <td style=\"text-align: center\">-50</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: center\">Diff between stateSOL and stateUser less than 0</td>\n",
    "        <td style=\"text-align: center\">-10</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Board: 3 x 3\n",
    "\n",
    "Number of Aquariums: 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aquarium-v2\n",
    " \n",
    "<table align=\"left\" style=\"width: 15em; margin: 1em 2em 2em 2em; border: 3px solid #f8f8f8;\">\n",
    "    <tr>\n",
    "        <th style=\"text-align: center\">Action</th>\n",
    "        <th style=\"text-align: center\">Reward</th>\n",
    "    <tr>\n",
    "        <td style=\"text-align: center\">Finish</td>\n",
    "        <td style=\"text-align: center\">+50</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: center\">OverFlow</td>\n",
    "        <td style=\"text-align: center\">-50</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: center\">Diff between stateSOL and stateUser less than 0</td>\n",
    "        <td style=\"text-align: center\">-10</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: center\">Diff between stateSOL and stateUser equal to 0</td>\n",
    "        <td style=\"text-align: center\">+10</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Board: 3 x 3\n",
    "\n",
    "Number of Aquariums: 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aquarium-v3\n",
    " \n",
    "<table align=\"left\" style=\"width: 15em; margin: 1em 2em 2em 2em; border: 3px solid #f8f8f8;\">\n",
    "    <tr>\n",
    "        <th style=\"text-align: center\">Action</th>\n",
    "        <th style=\"text-align: center\">Reward</th>\n",
    "    <tr>\n",
    "        <td style=\"text-align: center\">Finish</td>\n",
    "        <td style=\"text-align: center\">+50</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: center\">OverFlow</td>\n",
    "        <td style=\"text-align: center\">-50</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: center\">Diff between stateSOL and stateUser less than 0</td>\n",
    "        <td style=\"text-align: center\">-10</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Board: 4 x 4\n",
    "    \n",
    "Number of Aquariums: 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aquarium-v4\n",
    " \n",
    "<table align=\"left\" style=\"width: 15em; margin: 1em 2em 2em 2em; border: 3px solid #f8f8f8;\">\n",
    "    <tr>\n",
    "        <th style=\"text-align: center\">Action</th>\n",
    "        <th style=\"text-align: center\">Reward</th>\n",
    "    <tr>\n",
    "        <td style=\"text-align: center\">Finish</td>\n",
    "        <td style=\"text-align: center\">+50</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: center\">OverFlow</td>\n",
    "        <td style=\"text-align: center\">-50</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: center\">Diff between stateSOL and stateUser less than 0</td>\n",
    "        <td style=\"text-align: center\">-10</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: center\">Diff between stateSOL and stateUser equal to 0</td>\n",
    "        <td style=\"text-align: center\">+10</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Board: 4 x 4\n",
    "\n",
    "Number of Aquariums: 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aquarium-v5\n",
    " \n",
    "<table align=\"left\" style=\"width: 15em; margin: 1em 2em 2em 2em; border: 3px solid #f8f8f8;\">\n",
    "    <tr>\n",
    "        <th style=\"text-align: center\">Action</th>\n",
    "        <th style=\"text-align: center\">Reward</th>\n",
    "    <tr>\n",
    "        <td style=\"text-align: center\">Finish</td>\n",
    "        <td style=\"text-align: center\">+50</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: center\">OverFlow</td>\n",
    "        <td style=\"text-align: center\">-50</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: center\">Diff between stateSOL and stateUser less than 0</td>\n",
    "        <td style=\"text-align: center\">-10</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Board: 3 x 3\n",
    "\n",
    "Number of Aquariums: 3"
   ]
  },
  {
   "source": [
    "#### Aquarium-v6\n",
    " \n",
    "<table align=\"left\" style=\"width: 15em; margin: 1em 2em 2em 2em; border: 3px solid #f8f8f8;\">\n",
    "    <tr>\n",
    "        <th style=\"text-align: center\">Action</th>\n",
    "        <th style=\"text-align: center\">Reward</th>\n",
    "    <tr>\n",
    "        <td style=\"text-align: center\">Finish</td>\n",
    "        <td style=\"text-align: center\">+50</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: center\">OverFlow</td>\n",
    "        <td style=\"text-align: center\">-50</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: center\">Diff between stateSOL and stateUser less than 0</td>\n",
    "        <td style=\"text-align: center\">-10</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: center\">Diff between stateSOL and stateUser equal to 0</td>\n",
    "        <td style=\"text-align: center\">+10</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Board: 3 x 3\n",
    "\n",
    "Number of Aquariums"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each of these environments there are necessary functions to make all the RL Algorithms work. For instance, the basic ones: __init()__, __reset()__, __step()__ and __render()__.\n",
    "\n",
    "<b>Init: </b>As the name suggests, it initializes everything we need like randomly generating the board and defining the objectives as well as setting up all the variables needed for the rest of the program.\n",
    "\n",
    "<b>Reset: </b>This function is responsible for generating a new random game.\n",
    "\n",
    "<b>Step: </b>In this function, we do the play we were told by the action we received. The action it receives is an integer from 0 to n (number of containers) and we fill the specified aquarium on extra level, if it is possible at all.\n",
    "\n",
    "<b>Render: </b>This function is responsible for the display of info to the user. Throughout the development we changed this a lot to print useful information at each time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL Algorithms\n",
    "___\n",
    "\n",
    "Algorithms we implemented with different parameterizations:\n",
    "\n",
    "***1. [Q-Learning](#Q-Learning)***\n",
    "\n",
    "***2. [SARSA](#SARSA)***\n",
    "\n",
    "*** [Proximal Policy Optimization – PPO](#PPO)***\n",
    "\n",
    "We tried to implement the PPO algorithm, using OpenAI's baselines library, but we found that it wasn't viable to keep using the library without some major refactoring to our code, thus we decided to drop the idea of using the afformentioned library. And due to time constraints we were not able to implement this algorithm by scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning\n",
    "\n",
    "#### Steps\n",
    "```\n",
    "While the value of exp_exp_tradeoff is smaller than epsilon we choose a random action out of the action space (exploration):\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "Otherwise we choose the action that led to the biggest Q value for the current state (exploitation):\n",
    "    action = np.argmax(newQTable[state])\n",
    "\n",
    "This action will then be executed and we get a new state and a reward, which will be used to update the Q values: \n",
    "    new_state, reward, done, info = env.step(action)\n",
    "\n",
    "The values obtained by executing a step will now be used to update the Q-Table:\n",
    "    newQTable[state][action] = newQTable[state][action] + learning_rate * (reward + gamma * np.argmax(newQTable[new_state]) - newQTable[state][action])\n",
    "\n",
    "And lastly, the value of epsilon will be updated:\n",
    "\tepsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARSA\n",
    "\n",
    "This algorith is similar to Q-Learning, however the Q values represent the possible reward received in the next time step for taking action a in state s, plus the discounted future reward received from the next state-action observation.\n",
    "\n",
    "#### Steps\n",
    "```\n",
    "All the steps are similar to the Q-Learning algorith, but it requiers a one further action (new_action) and the state this action leads to (new_state), so the formula for updating the Q Values is altered to:\n",
    "\n",
    "newQTable[state][action] = newQTable[state][action] + learning_rate * (reward + gamma * newQTable[new_state][new_action] - newQTable[state][action])\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "<center><img src=\"images/q_sarsa_formula.png\" width=\"500px\" alt=\"SARSA\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO\n",
    "\n",
    "Proximal Policy Optimization, or PPO, is a policy gradient method for reinforcement learning. The motivation was to have an algorithm with the data efficiency and reliable performance of TRPO, while using only first-order optimization.\n",
    "\n",
    "In general use, PPO has a good performance and it's easy to use. However, we tried implemented this RL algorithm through the library called Stable Baselines 3 (https://stable-baselines3.readthedocs.io/en/master/), and the result was catastrophic as it would use up al the available RAM and end up crashing. \n",
    "\n",
    "After hours trying to do the algorithm, we quit because this library always call a function that, in our case, doesn't help to find the solution. The best option would try to implemented it by ourselves, but we hadn't time to finish it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparasion between Q-Learning and SARSA\n",
    "\n",
    "#### First Check\n",
    "With environment Aquarium-v0, with a board 4x4 and 4 aquariums, we tested if our Q-Learning and Sarsa implementations were functional using the same board. \n",
    "\n",
    "<center><img src=\"images/precision_sarsa_qlear.png\" width=\"500px\" alt=\"Precision\"/></center>\n",
    "\n",
    "To conclude, the algorithms seem to be functional.\n",
    "\n",
    "#### Changing Hyperparameters\n",
    "\n",
    "For environment \"Aquarium-v1\", with a board 3x3 and only 2 aquariums, we change each hyperparameter in order to obtain the best final precision.\n",
    "\n",
    "The default values are:\n",
    "- Number Episodes = 10000\n",
    "- Learning Rate = 0.9\n",
    "- Decay Rate = 0.001\n",
    "- Gamma = 0.9\n",
    "\n",
    "When running tests to one hyperparameter, we changed the value for that parameter, leaving the other three unchanged.\n",
    "\n",
    "The following graphs reflect the results of the tests we ran, the Y axis, represents the Test Precision. This value was obtained when the agent had finished training and then solved 1000 different puzzles. The number of puzzles solved successfully was then diveded by the number of total puzzles and thus we got the Test Precision value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<table align=\"center\">\n",
    "    <tr>\n",
    "        <th style=\"text-align: center\">Hyperparameter: Number of Episodes</th>\n",
    "        <th style=\"text-align: center\">Hyperparameter: Learning Rate</th>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"images/n_episodes.png\" width=\"500px\"/>\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"images/learning_rate.png\" width=\"500px\"/>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "<table align=\"center\">\n",
    "    <tr>\n",
    "        <th style=\"text-align: center\">Hyperparameter: Decay Rate</th>\n",
    "        <th style=\"text-align: center\">Hyperparameter: Gamma</th>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"images/decay_rate.png\" width=\"500px\"/>\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"images/gamma.png\" width=\"500px\"/>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "### Conclusions\n",
    "\n",
    "With the values we got and by interpreting the graphs we took the following conclusions:\n",
    "\n",
    "1. For both algorithms, if we increase the number of episodes, the test precision will be higher, this is to be expected, as the agent will have had more test boards to learn.\n",
    "\n",
    "2. For both algorithms, if we increase the learning rate, the test precision will decrease.\n",
    "\n",
    "3. For both algorithms, the best decay rate was 0.001, it was with that value that the algorithm had the best results.\n",
    "\n",
    "4. For Q Learning, gamma=0.5 is better. However, for Sarsa, it's preferable use gamma=0.25.\n",
    "\n",
    "\n",
    "#### Best Hyperparameters\n",
    "\n",
    "For Q Learning:\n",
    "* Number Episodes = 100000\n",
    "* Learning Rate = 0.25\n",
    "* Decay Rate = 0.001\n",
    "* Gamma = 0.5\n",
    "\n",
    "By training with a new Q-Table and then testing the agent with 1000 random puzzles, we obtained a test precision of 85.9%.\n",
    "\n",
    "In more detail, the values for epsilon and the total score per time by the end of these runs were:\n",
    "* Epsilon = 0.01\n",
    "* Score Per Time = -67.7627\n",
    "\n",
    "\n",
    "<table align=\"center\">\n",
    "    <tr>\n",
    "        <th style=\"text-align: center\">Total rewards per Number of Episodes (100000)</th>\n",
    "        <th style=\"text-align: center\">Total rewards per Number of Episodes (100)</th>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"images/q_learning_0.25_0.5_0.01.png\" width=\"500px\"/>\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"images/q_learning_0.25_0.5_0.9057890438555999.png\" width=\"500px\"/>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "For Sarsa:\n",
    "* Number Episodes = 100000\n",
    "* Learning Rate = 0.25\n",
    "* Decay Rate = 0.001\n",
    "* Gamma = 0.25\n",
    "\n",
    "By training with a new Sarsa Table and then testing the agent with 1000 random puzzles, we obtained a test precision of 84.5%.\n",
    "\n",
    "Once again, the values for epsilon and the total score per time by the end of these runs were:\n",
    "* Epsilon = 0.01\n",
    "* Score Per Time = -221.6181\n",
    "\n",
    "<table align=\"center\">\n",
    "    <tr>\n",
    "        <th style=\"text-align: center\">Total rewards per Number of Episodes (100000)</th>\n",
    "        <th style=\"text-align: center\">Total rewards per Number of Episodes (100)</th>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"images/sarsa_0.25_0.25_0.01.png\" width=\"500px\"/>\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"images/sarsa_0.25_0.25_0.9057890438555999.png\" width=\"500px\"/>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Overall, both Reinforcement Learning algorithms, worked pretty well for a simplified version of the puzzle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extrapolation to another environment\n",
    "\n",
    "To summarize our results within the different environments we present these tables which were used to compare the performance of each environment. For all this tests we used the best hyperparameters from the tests above, and with a number of episodes equal to 100 000.\n",
    "\n",
    "We have two kinds of rewards. We called one the \"Simple\" that was used on the V1, 3 and 5 environments while the V2, 4 and 6 used the \"Complex\" one. The following table explains these reward systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Simple reward</b>\n",
    " : : : : : : : : : :  : : : : : : : : : : : : : : : : : : : : VS : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :\n",
    "<b>Complex reward</b>\n",
    "<table align=\"left\" style=\"width: 15em; margin: 2em 2em 2em 2em; border: 3px solid #f8f8f8;\">\n",
    "    <tr>\n",
    "        <th style=\"text-align: center\">Action</th>\n",
    "        <th style=\"text-align: center\">Reward</th>\n",
    "    <tr>\n",
    "        <td style=\"text-align: center\">Finish</td>\n",
    "        <td style=\"text-align: center\">+50</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: center\">OverFlow</td>\n",
    "        <td style=\"text-align: center\">-50</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: center\">Diff between stateSOL and stateUser less than 0</td>\n",
    "        <td style=\"text-align: center\">-10</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "<table align=\"right\" style=\"width: 15em; margin: 1em 2em 2em 2em; border: 3px solid #f8f8f8;\">\n",
    "    <tr>\n",
    "        <th style=\"text-align: center\">Action</th>\n",
    "        <th style=\"text-align: center\">Reward</th>\n",
    "    <tr>\n",
    "        <td style=\"text-align: center\">Finish</td>\n",
    "        <td style=\"text-align: center\">+50</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: center\">OverFlow</td>\n",
    "        <td style=\"text-align: center\">-50</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: center\">Diff between stateSOL and stateUser less than 0</td>\n",
    "        <td style=\"text-align: center\">-10</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: center\">Diff between stateSOL and stateUser equal to 0</td>\n",
    "        <td style=\"text-align: center\">+10</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next table shows the performance of each environment. Within the table each field has two percentages that correspond to the precision of the Q-Learning and the Sarsa algorithms, in that order (QL/SARSA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"centre\" style=\"width: 45em; margin: 1em 4em 4em 1em; border: 9px solid #f8f8f8;\">\n",
    "    <tr>\n",
    "        <th style=\"text-align: center\"><b>Board size, # Aquariums</b></th>\n",
    "        <th style=\"text-align: center\"><b>Simple</b></th>\n",
    "        <th style=\"text-align: center\"><b>Complex</b></th>\n",
    "    <tr>\n",
    "        <td style=\"text-align: center\">3,   2</td>\n",
    "        <td style=\"text-align: center\">85.9% / 84.5%</td>\n",
    "        <td style=\"text-align: center\">84.5% / 84.4%</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: center\">3,   3</td>\n",
    "        <td style=\"text-align: center\">41.0% / 34.0%</td>\n",
    "        <td style=\"text-align: center\">45.7% / 42.2%</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: center\">4,   2</td>\n",
    "        <td style=\"text-align: center\">87.7% / 89.9%</td>\n",
    "        <td style=\"text-align: center\">86.9% / 89.4%</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, we can see that our implementation had greater precision on puzzles with 2 aquariums than when the board had 3 aquariums. We beleive that happened because more aquariums imply a bigger action and observation space thus it is more computationally heavy, which makes it more difficult for our RL agent to find a solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning for Aquarium\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import gym_game\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "\n",
    "env = gym.make(\"Aquarium-v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Q-table\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# Define number of aquariums\n",
    "n = 2\n",
    "\n",
    "newQTable = {}\n",
    "\n",
    "print(f'action size: {action_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @hyperparameters\n",
    "total_episodes = 10000       # Total episodes\n",
    "learning_rate = 0.25         # Learning rate\n",
    "max_steps = 16               # Max steps per episode\n",
    "gamma = 0.5                  # Discounting rate\n",
    "\n",
    "# Exploration parameters\n",
    "epsilon = 0.9                # Exploration rate\n",
    "max_epsilon = 1.0            # Exploration probability at start\n",
    "min_epsilon = 0.01           # Minimum exploration probability\n",
    "decay_rate = 0.001           # Exponential decay rate for exploration prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary function to choose an action\n",
    "def choose_action(exp_exp_tradeoff,epsilon,newQTable,state,env):\n",
    "\t## If this number > greater than epsilon --> exploitation\n",
    "\t#(taking the biggest Q value for this state)\n",
    "\tif exp_exp_tradeoff > epsilon:\n",
    "\t\ttry:\n",
    "\t\t\taction = np.argmax(newQTable[state])\n",
    "\t\texcept:\n",
    "\t\t\tnewQTable[state] = [0 for i in range(n)]\n",
    "\t\t\taction = np.argmax(newQTable[state])\n",
    "\n",
    "\n",
    "\t# Else doing a random choice --> exploration\n",
    "\telse:\n",
    "\t\taction = env.action_space.sample()\n",
    "\t# print(action)\n",
    "\treturn action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# Learn through Q-learning\n",
    "\n",
    "# List of rewards\n",
    "rewards = []\n",
    "\n",
    "state = env.reset()\n",
    "# print(state)\n",
    "\n",
    "# ----------------------------------------------\n",
    "# ------------------ Training ------------------\n",
    "# ----------------------------------------------\n",
    "\n",
    "# 2 For life or until learning is stopped\n",
    "for episode in range(total_episodes):\n",
    "\t# Reset the environment\n",
    "\tstate = env.reset()\n",
    "\n",
    "\t# If the state isn't already in the q-table then we must add it and initialize the q-values for that state as 0\n",
    "\ttry:\n",
    "\t\tnewQTable[state]\n",
    "\texcept:\n",
    "\t\tnewQTable[state] = [0 for i in range(n)]\n",
    "\n",
    "\t# print(f\"state: {state}\")\n",
    "\tstep = 0\n",
    "\tdone = False\n",
    "\ttotal_rewards = 0\n",
    "\n",
    "\n",
    "\n",
    "\tfor step in range(max_steps):\n",
    "\t\t# print(f\"start step...\")\n",
    "\t\t# 3. Choose an action a in the current world state (s)\n",
    "\t\t## First we randomize a number\n",
    "\t\texp_exp_tradeoff = random.uniform(0, 1)\n",
    "\n",
    "\t\t# Choose an action \n",
    "\t\taction = choose_action(exp_exp_tradeoff,epsilon,newQTable,state,env)\n",
    "\t\t\n",
    "\n",
    "\t\t# Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "\t\tnew_state, reward, done, info = env.step(action)\n",
    "\t\t# If the new state is equal to the old state then choose a new random move to prevent getting stuck\n",
    "\t\tif new_state == state:\n",
    "\t\t\tend_state = \"\"\n",
    "\t\t\tfor x in env.pygame.aqDepth:\n",
    "\t\t\t\tend_state = end_state + str(x)\n",
    "\n",
    "\t\twhile (new_state == state and new_state[:n] != end_state):\n",
    "\t\t\taction = env.action_space.sample()\n",
    "\t\t\tnew_state, reward, done, info = env.step(action)\n",
    "\n",
    "\t\ttry:\n",
    "\t\t\tnewQTable[new_state]\n",
    "\t\texcept:\n",
    "\t\t\tnewQTable[new_state] = [0 for i in range(n)]\n",
    "\n",
    "\n",
    "\t\t# print(new_state)\n",
    "\n",
    "\t\t# Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "\t\tnewQTable[state][action] = newQTable[state][action] + learning_rate * (reward + gamma * np.argmax(newQTable[new_state]) - newQTable[state][action])\n",
    "\n",
    "\n",
    "\t\ttotal_rewards = total_rewards + reward\n",
    "\n",
    "\t\t# Our new state is state\n",
    "\t\told_state = state\n",
    "\t\tstate = new_state\n",
    "\n",
    "\t\n",
    "\n",
    "\t\t# If done (if we're dead) : finish episode\n",
    "\t\tif done == True:\n",
    "\t\t\tbreak\n",
    "\n",
    "\tepisode += 1\n",
    "\t# Reduce epsilon (because we need less and less exploration)\n",
    "\tepsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
    "\trewards.append(total_rewards)\n",
    "\n",
    "print(\"----------------------------\")\n",
    "print(\"Score/time: \" +  str(sum(rewards)/total_episodes))\n",
    "print(\"----------------------------\")\n",
    "print(\"     \t Q-Table\t\t   \")\n",
    "print(\"----------------------------\")\n",
    "for i in sorted(newQTable.keys()):\n",
    "\tprint('State', i, ':' ,newQTable[i])\n",
    "# print(newQTable)\n",
    "print(\"Epsilon: \",epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rewards)\n",
    "plt.ylabel('Total Rewards')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploit!\n",
    "\n",
    "#All episodes are taking the maximum of Qtable value\n",
    "state = env.reset()\n",
    "old_state = \"\"\n",
    "old_action = 0\n",
    "steps = 0\n",
    "done = False\n",
    "correct = 0\n",
    "print(\"----------------------------\")\n",
    "for episodes in range(1000):\n",
    "\tsteps = 0\n",
    "\tdone = False\n",
    "\tstate = env.reset()\n",
    "\twhile not done and steps<max_steps:\n",
    "\t\tsteps += 1\n",
    "\t\ttry:\n",
    "\t\t\taction = np.argmax(newQTable[state])\n",
    "\t\texcept:\n",
    "\t\t\tnewQTable[state] = [0 for i in range(n)]\n",
    "\t\t\taction = np.argmax(newQTable[state])\n",
    "\n",
    "\t\tnew_state, _, done, _ = env.step(action)\n",
    "\n",
    "\t\tstate = new_state\n",
    "\t\tif done:\n",
    "\t\t\tcorrect += 1\n",
    "\t\t\n",
    "print(\"Percentage of puzzles solved correctly: \",(correct/1000)*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}